{"posts":[{"title":"mysql无损/半同步复制","content":"1.参数文件里加 plugin_dir=/usr/local/mysql/lib/plugin plugin_load = &quot;rpl_semi_sync_master=semisync_master.so;rpl_semi_sync_slave=semisync_slave.so&quot; loose_rpl_semi_sync_master_enabled = 1 loose_rpl_semi_sync_slave_enabled = 1 loose_rpl_semi_sync_master_timeout = 5000 2.主库准备步骤 grant replication slave, replication client on *.* to rep@'192.168.56.%' identified by '123456'; flush privileges; mysql&gt; show master status; +------------+----------+--------------+------------------+----------------------------------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------+----------+--------------+------------------+----------------------------------------------+ | bin.000007 | 57566891 | | | 35d3ad22-8862-11ea-97a1-080027bb5279:1-43998 | +------------+----------+--------------+------------------+----------------------------------------------+ 3.备库准备步骤 change master to master_host='192.168.56.101',master_user='rep',master_password='123456',master_log_file='bin.000007', master_log_pos=57566891,master_port = 3306; 4.测试 # 主库执行 mysql&gt; show variables like '%rpl%'; +-------------------------------------------+------------+ | Variable_name | Value | +-------------------------------------------+------------+ | rpl_semi_sync_master_enabled | ON | | rpl_semi_sync_master_timeout | 5000 | | rpl_semi_sync_master_trace_level | 32 | | rpl_semi_sync_master_wait_for_slave_count | 1 | | rpl_semi_sync_master_wait_no_slave | ON | | rpl_semi_sync_master_wait_point | AFTER_SYNC | | rpl_semi_sync_slave_enabled | ON | | rpl_semi_sync_slave_trace_level | 32 | | rpl_stop_slave_timeout | 31536000 | +-------------------------------------------+------------+ 9 rows in set (0.00 sec) mysql&gt; show global status like &quot;%rpl%&quot;; +--------------------------------------------+-------+ | Variable_name | Value | +--------------------------------------------+-------+ | Rpl_semi_sync_master_clients | 1 | | Rpl_semi_sync_master_net_avg_wait_time | 0 | | Rpl_semi_sync_master_net_wait_time | 0 | | Rpl_semi_sync_master_net_waits | 0 | | Rpl_semi_sync_master_no_times | 1 | | Rpl_semi_sync_master_no_tx | 2 | | Rpl_semi_sync_master_status | ON | | Rpl_semi_sync_master_timefunc_failures | 0 | | Rpl_semi_sync_master_tx_avg_wait_time | 0 | | Rpl_semi_sync_master_tx_wait_time | 0 | | Rpl_semi_sync_master_tx_waits | 0 | | Rpl_semi_sync_master_wait_pos_backtraverse | 0 | | Rpl_semi_sync_master_wait_sessions | 0 | | Rpl_semi_sync_master_yes_tx | 0 | | Rpl_semi_sync_slave_status | OFF | +--------------------------------------------+-------+ 15 rows in set (0.00 sec) # 从库执行 mysql&gt; show global variables like '%semi%'; +-------------------------------------------+------------+ | Variable_name | Value | +-------------------------------------------+------------+ | rpl_semi_sync_master_enabled | ON | | rpl_semi_sync_master_timeout | 5000 | | rpl_semi_sync_master_trace_level | 32 | | rpl_semi_sync_master_wait_for_slave_count | 1 | | rpl_semi_sync_master_wait_no_slave | ON | | rpl_semi_sync_master_wait_point | AFTER_SYNC | | rpl_semi_sync_slave_enabled | ON | | rpl_semi_sync_slave_trace_level | 32 | +-------------------------------------------+------------+ 8 rows in set (0.00 sec) mysql&gt; stop slave io_thread; Query OK, 0 rows affected (0.00 sec) # 主库执行 mysql&gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | +--------------------+ 4 rows in set (0.00 sec) mysql&gt; create database test; Query OK, 1 row affected (5.00 sec) ---(rpl_semi_sync_master_timeout设置的5秒超时) mysql&gt; show global status like &quot;%rpl%&quot;; +--------------------------------------------+-------+ | Variable_name | Value | +--------------------------------------------+-------+ | Rpl_semi_sync_master_clients | 0 | | Rpl_semi_sync_master_net_avg_wait_time | 0 | | Rpl_semi_sync_master_net_wait_time | 0 | | Rpl_semi_sync_master_net_waits | 0 | | Rpl_semi_sync_master_no_times | 2 | | Rpl_semi_sync_master_no_tx | 3 | | Rpl_semi_sync_master_status | OFF | | Rpl_semi_sync_master_timefunc_failures | 0 | | Rpl_semi_sync_master_tx_avg_wait_time | 0 | | Rpl_semi_sync_master_tx_wait_time | 0 | | Rpl_semi_sync_master_tx_waits | 0 | | Rpl_semi_sync_master_wait_pos_backtraverse | 0 | | Rpl_semi_sync_master_wait_sessions | 0 | | Rpl_semi_sync_master_yes_tx | 0 | | Rpl_semi_sync_slave_status | OFF | +--------------------------------------------+-------+ 15 rows in set (0.00 sec) # 从库执行 start slave io_thread; # 主库执行 mysql&gt; show global status like &quot;%rpl%&quot;; +--------------------------------------------+-------+ | Variable_name | Value | +--------------------------------------------+-------+ | Rpl_semi_sync_master_clients | 1 | | Rpl_semi_sync_master_net_avg_wait_time | 0 | | Rpl_semi_sync_master_net_wait_time | 0 | | Rpl_semi_sync_master_net_waits | 1 | | Rpl_semi_sync_master_no_times | 2 | | Rpl_semi_sync_master_no_tx | 3 | | Rpl_semi_sync_master_status | ON | | Rpl_semi_sync_master_timefunc_failures | 0 | | Rpl_semi_sync_master_tx_avg_wait_time | 0 | | Rpl_semi_sync_master_tx_wait_time | 0 | | Rpl_semi_sync_master_tx_waits | 0 | | Rpl_semi_sync_master_wait_pos_backtraverse | 0 | | Rpl_semi_sync_master_wait_sessions | 0 | | Rpl_semi_sync_master_yes_tx | 0 | | Rpl_semi_sync_slave_status | OFF | +--------------------------------------------+-------+ 15 rows in set (0.00 sec) ","link":"https://hkxtor.github.io/BwozcZWlU/"},{"title":"MySQL自带的性能压力测试工具mysqlslap","content":"--only-print 只打印测试语句而不实际执行。 --detach=N 执行N条语句后断开重连。 # 单线程测试。测试做了什么。 mysqlslap -a -uroot -p123456 # 多线程测试。使用–concurrency来模拟并发连接。 mysqlslap -a -c 100 -uroot -p123456 # 迭代测试。用于需要多次执行测试得到平均值。 mysqlslap -a -i 10 -uroot -p123456 mysqlslap --auto-generate-sql-add-autoincrement -a -uroot -p123456 mysqlslap -a --auto-generate-sql-load-type=mixed -uroot -p123456 mysqlslap -a --auto-generate-sql-secondary-indexes=3 -uroot -p123456 mysqlslap -a --auto-generate-sql-write-number=1000 -uroot -p123456 mysqlslap --create-schema world -q &quot;select count(*) from City&quot; -uroot -p123456 mysqlslap -a -e innodb -uroot -p123456 mysqlslap -a --number-of-queries=10 -uroot -p123456 # 测试同时不同的存储引擎的性能进行对比： mysqlslap -a --concurrency=50,100 --number-of-queries 1000 --iterations=5 --engine=myisam,innodb --debug-info -uroot -p123456 # 500和1000个并发分别得到一次测试结果(Benchmark)，并发数越多，执行完所有查询的时间越长。为了准确起见，可以多测试几次: mysqlslap -a --concurrency=500,1000 --auto-generate-sql-add-autoincrement --auto-generate-sql-secondary-indexes=10 --number-of-queries 2000 --iterations=5 Benchmark Average number of seconds to run all queries: 1.551 seconds Minimum number of seconds to run all queries: 1.441 seconds Maximum number of seconds to run all queries: 1.716 seconds Number of clients running queries: 500 Average number of queries per client: 4 Benchmark Average number of seconds to run all queries: 1.734 seconds Minimum number of seconds to run all queries: 1.589 seconds Maximum number of seconds to run all queries: 1.911 seconds Number of clients running queries: 1000 Average number of queries per client: 2 ","link":"https://hkxtor.github.io/DQakzE_dy/"},{"title":"12c可插拔数据库的几种克隆迁移方法","content":"Oacle 多租户环境包含一个容器数据库(CDB)和零个或多个可插拔数据库(PDB)，这种让数据库系统扩展也变得非常的灵活，oracle 12c提供了许多种关于多租户模式下数据库的克隆迁移方式，以下对于几种克隆迁移的方式进行实验介绍。 一．通过现有PDB直接创建（CREATE PLUGGABLE DATABASE..FROM..） 从本地PDB创建 克隆完整PDB 创建语句： CREATE PLUGGABLE DATABASE pdb3 FROM pdb2 PATH_PREFIX = ' /u01/app/oracle/oradata/ ' --- 指定pdb相关联的目录，可以不设置 FILE_NAME_CON VERT = (' /u01/app/oracle/oradata/orclcwd/pdb2/', '/u01/app/oracle/oradata/orclcwd/pdb3/' ) - - 数据文件存放路径，在ASM中可以指定DG SERVICE_NAME_CONVERT = ('pdb2','pdb3') – 服务名转换 NOLOGGING; CREATE PLUGGABLE DATABASE pdb3 FROM pdb2 PATH_PREFIX = '/u01/app/oracle/oradata/' FILE_NAME_CONVERT = ('/u01/app/oracle/product/12.1.0/dbhome_1/dbs/u01/app/oracle/oradata/orclcwd/', '/u01/app/oracle/oradata/orclcwd/pdb3/') SERVICE_NAME_CONVERT = ('pdb2','pdb3'); 主要的选项说明： PATH_PREFIX：将PDB的相对目录对象路径设置为特定目录。因此，需要设置PATH_PREFIX FILE_NAME_CONVERT：指定数据文件的转换路径 Storage: 如果需要限制新建pdb的大小，可以使用storage=xxG 来限制。 创建的过程日志： CREATE PLUGGABLE DATABASE pdb3 FROM pdb2 PATH_PREFIX = '/u01/app/oracle/oradata/' FILE_NAME_CONVERT = ('/u01/app/oracle/product/12.1.0/dbhome_1/dbs/u01/app/oracle/oradata/orclcwd/', '/u01/app/oracle/oradata/orclcwd/pdb3/') SERVICE_NAME_CONVERT = ('pdb2','pdb3') &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; 创建命令开始 Fri Aug 18 14:22:11 2017 Opatch XML is skipped for PDB PDB2 (conid=3) &lt;&lt;&lt;&lt;&lt; 跳过PDB2（源）Opatch XML，看起来与版本检查有关 APEX_040200.WWV_FLOW_ADVISOR_CHECKS (CHECK_STATEMENT) - CLOB populated Fri Aug 18 14:24:15 2017 &lt;&lt;&lt;&lt; 开始创建新的PDB3，但此时的状态先为UNUSABLE **************************************************************** Pluggable Database PDB3 with pdb id - 4 is created as UNUSABLE. If any errors are encountered before the pdb is marked as NEW, then the pdb must be dropped **************************************************************** Database Characterset for PDB3 is ZHS16GBK &lt;&lt;&lt; 字符集检查 Fri Aug 18 14:24:26 2017 &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; 删除换旧的数据文件并生成新的数据文件 Deleting old file#8 from file$ Deleting old file#9 from file$ Adding new file#11 to file$(old file#8) Adding new file#12 to file$(old file#9) Successfully created internal service pdb3 at open &lt;&lt;&lt;&lt;&lt;&lt; 创建服务成功 ALTER SYSTEM: Flushing buffer cache inst=0 container=4 local &lt;&lt;&lt;&lt;&lt; 刷新新PDB的buffer **************************************************************** Post plug operations are now complete. &lt;&lt;&lt;&lt;&lt; 插入PDB操作 Pluggable database PDB3 with pdb id - 4 is now marked as NEW. &lt;&lt;&lt;&lt;PDB 状态定位NEW **************************************************************** Completed: CREATE PLUGGABLE DATABASE pdb3 FROM pdb2 &lt;&lt;&lt;&lt; 完成 PATH_PREFIX = '/u01/app/oracle/oradata/' FILE_NAME_CONVERT = ('/u01/app/oracle/product/12.1.0/dbhome_1/dbs/u01/app/oracle/oradata/orclcwd/', '/u01/app/oracle/oradata/orclcwd/pdb3/') SERVICE_NAME_CONVERT = ('pdb2','pdb3') 仅克隆PDB元数据 有时并不需要克隆表里的数据，可使用NO DATA来克隆一个PDB，但仅仅克隆元数据。 以下示例： 源PDB中表存在多行数据，这里特意选择了三种情况，看看是否NO DATA的机制如何： SYS 用户的表，表空间在SYSTEM 个人用户的表，表空间在SYSTEM 个人用户的表，表空间在普通表空间 将源PDB打开到read only模式下： ALTER PLUGGABLE DATABASE pdb1 OPEN READ ONLY; 克隆PDB： CREATE PLUGGABLE DATABASE pdb4 FROM pdb3 NO DATA FILE_NAME_CONVERT = ('/u01/app/oracle/oradata/orclcwd/pdb3/', '/u01/app/oracle/oradata/orclcwd/pdb4/'); SQL&gt; CREATE PLUGGABLE DATABASE pdb4 FROM pdb3 NO DATA; CREATE PLUGGABLE DATABASE pdb4 FROM pdb3 NO DATA * ERROR at line 1: ORA-65016: FILE_NAME_CONVERT must be specified SQL&gt; CREATE PLUGGABLE DATABASE pdb4 FROM pdb3 NO DATA 2 FILE_NAME_CONVERT = ('/u01/app/oracle/oradata/orclcwd/pdb3/','/u01/app/oracle/oradata/orclcwd/pdb4/'); Pluggable database created. 打开新克隆的PDB： ALTER PLUGGABLE DATABASE pdb4 OPEN; 到新的PDB下查询表数据： 从查询结果来看，使用NO DATA的方式克隆PDB时，SYSTEM表空间下的表数据是会克隆过去，但用户表空间下表数据库就仅克隆了元数据。 从远程PDB或non-CDB创建 通过远程方式创建克隆主要依靠的是dblink，因此需要源库和目标库之间网络保持畅通，而远处创建的方式和本地创建相差不大，远程模式可以增加从一个NON-CDB数据库克隆到CDB中。 从PDB创建 查看目标CDB当前情况： 在目标CDB创建连接远程CDB的dblink： SQL&gt; create public database link cdb1 connect to system identified by &quot;111111&quot; using 'cdb1'; Database link created. 使用dblink远程创建 SQL&gt; CREATE PLUGGABLE DATABASE pdb5 FROM pdb4@cdb1 2 FILE_NAME_CONVERT = ('/u01/app/oracle/oradata/orclcwd/pdb4/','/u01/app/oracle/oradata/orclcwd/pdb5/'); Pluggable database created. 完成创建： 从NOCDB创建 在目标CDB创建连接远程非CDB的dblink： SQL&gt; create public database link nocdb1 connect to system identified by &quot;111111&quot; using 'nocdb1'; Database link created. 使用dblink远程创建 SQL&gt; CREATE PLUGGABLE DATABASE pdb6 FROM nocdb1@nocdb1 2 FILE_NAME_CONVERT = ('/u01/app/oracle/oradata/nocdb1/','/u01/app/oracle/oradata/orclcwd/pdb6/'); Pluggable database created. 创建完成 执行PDB转换脚本 SQL&gt; alter session set container=pdb6; SQL&gt; @?/rdbms/admin/ noncdb_to_pdb.sql Plug 过程的一些问题： 通过PDB_PLUG_IN_VIOLATIONS视图可以查询plug过程中的问题，例如以下，源PDB和当前CDB的字符集不同导致PDB open 后是限制模式，这个可以通过 ALTER DATABASE CHARACTER SET internal_use ZHS16GBK; 更改，如果在 实际应 用中就要事先注意 检查 字符集是否相同。 二.通过插拔方式创建（CREATE PLUGGABLE DATABASE..USING ‘XML’..） 插拔的方式，是将一个PDB从CDB中拔出，之后插入到另外一个CDB的过程，一个拔掉的 PDB 由描述 PDB 的 XML 文件和其相关数据文件 组 成。 从PDB插拔 选择准备拔出的 PDB ，这里选择 PDB4. 关闭 PDB 生成PDB的描述XML文件 ALTER PLUGGABLE DATABASE PDB4 UNPLUG INTO '/home/oracle/PDB4.xml'; 将PDB4的文件传输到目标库相同路径下： 将生成的XML文件传输到目标库 执行DBMS_PDB.CHECK_PLUG_COMPATIBILITY 检查要插入PDB是否和目标CDB兼容。 SET SERVEROUTPUT ON DECLARE compatible CONSTANT VARCHAR2(3) := CASE DBMS_PDB.CHECK_PLUG_COMPATIBILITY( pdb_descr_file =&gt; '/home/oracle/PDB4.xml') WHEN TRUE THEN 'YES' ELSE 'NO' END; BEGIN DBMS_OUTPUT.PUT_LINE(compatible); END; / SQL&gt; SET SERVEROUTPUT ON SQL&gt; DECLARE 2 compatible CONSTANT VARCHAR2(3) := 3 CASE DBMS_PDB.CHECK_PLUG_COMPATIBILITY( 4 pdb_descr_file =&gt; '/home/oracle/PDB4.xml') 5 WHEN TRUE THEN 'YES' 6 ELSE 'NO' 7 END; 8 BEGIN 9 DBMS_OUTPUT.PUT_LINE(compatible); 10 END; 11 / NO &lt;&lt;&lt;&lt; 检查不通过 ，原来出现字符集不同 SQL&gt; ALTER SYSTEM ENABLE RESTRICTED SESSION; System altered. SQL&gt; ALTER DATABASE CHARACTER SET internal_use ZHS16GBK; Database altered. SQL&gt; SET SERVEROUTPUT ON SQL&gt; DECLARE 2 compatible CONSTANT VARCHAR2(3) := 3 CASE DBMS_PDB.CHECK_PLUG_COMPATIBILITY( 4 pdb_descr_file =&gt; '/home/oracle/PDB4.xml') 5 WHEN TRUE THEN 'YES' 6 ELSE 'NO' 7 END; 8 BEGIN 9 DBMS_OUTPUT.PUT_LINE(compatible); 10 END; 11 / YES &lt;&lt;&lt;&lt; 检查 通 过 PL/SQL procedure successfully completed. 对于插入一个PDB，必须满足以下条件： 目标CDB必须与源CDB具有相同的字节码（主要涉及平台问题） 。 CDB必须安装相同的选项 。 源CDB和目标CDB必须具有兼容的字符集和国家字符集。 使用XML元数据文件把PDB插入（克隆模式） create pluggable database MYPDB3 AS CLONE using '/home/oracle/PDB4.xml' NOCOPY TEMPFILE REUSE; 这里使用NOCOPY是因为将目标库数据文件路径设成了与源库相同。 打开新的插入的PDB,但出现错误 SQL&gt; !oerr ora 65054 65054, 00000, &quot;Cannot open a pluggable database in the desired mode.&quot; // *Cause: An attempt was made to open a pluggable database in a mode // incompatible with that of the CDB. // *Action: Open the CDB in a compatible mode first and retry the operation. 看起来是新的PDB和CDB OPEN时不兼容，这里我尝试重启整个CDB后竟然可以顺利打开新插入的PDB了。 从 NON-CDB 插拔 从NON-CDB转换克隆的模式实际与从CDB相差不大，与从现有NOCDB创建的模式一样，这里也需要执行noncdb_to_pdb.sql脚本来转换数据字典等。 以下列举出执行的语句： 关闭原NONCDB数据库,并开启到只读模式 sqlplus / as sysdba sql&gt; shutdown immediate sql&gt; startup open read only 生成数据库的XML元数据文件。 BEGIN DBMS_PDB.DESCRIBE(pdb_descr_file =&gt; '/home/oracle/12cNonPDB.xml'); END; / 关闭数据库 sql&gt; shutdown immediate 同样将原数据库文件传输到新数据库磁盘中。 检查兼容性 SET SERVEROUTPUT ON; DECLARE compatible CONSTANT VARCHAR2(3) := CASE DBMS_PDB.CHECK_PLUG_COMPATIBILITY(pdb_descr_file =&gt; '/home/oracle/12cNonPDB.xml') WHEN TRUE THEN 'YES' ELSE 'NO' END; BEGIN DBMS_OUTPUT.PUT_LINE(compatible); END; / 检查是否有错误 col cause for a20 col name for a20 col message for a35 word_wrapped select name,cause,type,message,status from PDB_PLUG_IN_VIOLATIONS where name='&lt;noncdb database name&gt;'; 创建PDB CREATE PLUGGABLE DATABASE PDB8 USING ' /home/oracle/12cNonPDB.xml' COPY FILE_NAME_CONVERT = ('/u01/app/oracle/12c/oradata/12cNonPDB/','/u01/app/oracle/oradata/12c/PDB8/'); 执行转换脚本 sql&gt; ALTER SESSION SET CONTAINER= PDB8; sql&gt; @$ORACLE_HOME/rdbms/admin/noncdb_to_pdb.sql 打开新的PDB ALTER PLUGGABLE DATABASE PDB8 OPEN; 三．通过RMAN备份恢复 整体数据库备份恢复 有时可能需要迁移的仅仅是其中某几个PDB，在整库数据量不大情况下，可以直接备份整库来异机恢复即可，这样和普通的的数据库模式是相同的。 整库备份 run{ allocate channel CH1 device type disk format '/home/oracle/backup/full_db_%U'; backup database include current controlfile plus archivelog ; release channel CH1 ; } BACKUP current controlfile format '/home/oracle/backup/control_%d_%T_%u.ctl'; 传输备份片到目标库 [oracle@redhat1 ~]$ pwd /home/oracle [oracle@redhat1 ~]$ scp -r backup 172.16.155.67:/home/oracle/ 恢复控制文件 Startup nomout restore controlfile from '/home/oracle/backup/control_ORCLCWD_20170819_1esc91u4.ctl'; alter database mount; 恢复全库 run{ allocate channel CH1; restore database ; recover database; release channel CH1 ; } alter database open resetlogs; 只恢复CDB、pdb$seed和需要的PDB 备份数据库 这里使用上一步骤的全库备份，这里有个问题，就是我只需要恢复其中某一个PDB，那么，是否可以直接备份root database ,种子pdb 和所要的pdb来单独恢复，这个在后面进行详细实验。 创建参数文件. 这个不做说明 恢复控制文件 startup nomout restore controlfile from '/home/oracle/backup/control_ORCLCWD_20170819_1esc91u4.ctl'; alter database mount; 恢复所需要的 CDB$ROOT,PDB$SEED和PDB4 run{ ALLOCATE CHANNEL c1 TYPE disk; restore database root ; -------------------------&gt;CDB$ROOT restore database &quot;PDB$SEED&quot;; --------&gt;PDB$SEED is required restore database PDB4; --------------&gt;PDB we want to restore RELEASE CHANNEL c1; } 如果源库和目标库的数据文件路径不同，这使用set new name方式来更改文件路径。 跳过不需要的PDB的表空间来做recover. 在本示例中， 数据库中除了CDB$ROOT和PDB$SEED，还有PDB2和PDB4，选择恢复的是PDB4，因此PDB2在recover时必须进行排除。 run{ recover database skip forever tablespace PDB2:SYSTEM,PDB2:SYSAUX; } 打开数据库 打开后查看发现有pdb2，这个实际时不可用的，且没有实际数据，直接drop掉即可。 SQL&gt; show pdbs CON_ID CON_NAME OPEN MODE RESTRICTED ---------- ------------------------------ ---------- ---------- 2 PDB$SEED MOUNTED 3 PDB2 MOUNTED 5 PDB4 MOUNTED SQL&gt; alter pluggable database PDB2 open; alter pluggable database PDB2 open * ERROR at line 1: ORA-01147: SYSTEM tablespace file 8 is offline SQL&gt; drop pluggable database pdb2 including datafiles; Pluggable database dropped. 而在打开PDB4时报出了ORA-65086，这里可以试下插拔一次。 SQL&gt; alter pluggable database pdb4 open; alter pluggable database pdb4 open * ERROR at line 1: ORA-65086: cannot open/close the pluggable database SQL&gt; alter pluggable database pdb4 unplug into '/tmp/pdb41.xml'; Pluggable database altered. 1* select pdb_name,status from dba_pdbs SQL&gt; / PDB_NAME STATUS -------------------- --------- PDB$SEED NEW PDB4 UNPLUGGED SQL&gt; drop pluggable database pdb4 keep datafiles; Pluggable database dropped. SQL&gt; select pdb_name,status from dba_pdbs; PDB_NAME STATUS -------------------- --------- PDB$SEED NEW SQL&gt; create pluggable database PDB4 using '/tmp/pdb41.xml' nocopy tempfile reuse; Pluggable database created. SQL&gt; select pdb_name,status from dba_pdbs; PDB_NAME STATUS -------------------- --------- PDB4 NEW PDB$SEED NEW SQL&gt; alter pluggable database pdb4 open; Pluggable database altered. SQL&gt; show pdbs CON_ID CON_NAME OPEN MODE RESTRICTED ---------- ------------------------------ ---------- ---------- 2 PDB$SEED MOUNTED 3 PDB4 READ WRITE NO 经过测试，可以只对需要的PDB进行备份后进行恢复即可,在迁移时并不需要整个库都备份，这样的话可以节省不少时间和空间。 run{ allocate channel CH1 device type disk format '/home/oracle/backup/full_db_%U'; backup database root include current controlfile plus archivelog; backup database &quot;PDB$SEED&quot;; backup pluggable database PDB4 include current controlfile plus archivelog; release channel CH1 ; } BACKUP current controlfile format '/home/oracle/backup/control_%d_%T_%u.ctl'; 后面恢复的方法与以上相同。 ","link":"https://hkxtor.github.io/qulIB3Rk0/"},{"title":"如何判断数据库IO是否慢","content":"1. 引言 一个项目的数据库响应慢的问题，结论就是操作系统层面上的IO性能差的问题，怎么样才能更有说服力。 我们一般衡量IO性能主要用两种指标来衡量。 响应时间：用微秒来测量完成一项操作所需的时间，这个一般由oracle来采集统计。 吞吐量：以每个单位时间内完成的操作数量来测量。这个一般通过操作系统下的工具来统计，例如iostat。 本文主要是阐述如何从oracle的角度来确定IO是否慢，不再详细说明吞吐量及其测量方法。 2. 怎么才算IO“慢” IO慢是一个很主观的术语，它更多的取决于用户对系统和硬件的预期和实际的差异，这个差异是一个比较主观的感觉，量化到具体性能指标来看，在企业级平台下，当IO请求的响应时间大于10ms时，用户会开始敏感。不过响应时间是会不断变化的，有可能是数据从文件系统迁移到共享存储上，也可能是存储设备出现异常，又或者是业务增长导致的IO性能达到峰值。这就需要通过对响应时间这个指标作出作出多维度的测量。 3. 响应时间 硬件不必对于每个IO请求都有相同的反映。总会有可能出现高峰和低谷。因此使用平均值是一种测量响应时间的通用方法。 注意：为了减缓这种高峰/低谷的异常场景带来的问题，样例数据量需要比较大。样例数据量应该至少是每小时1000次操作，目的就是为了提供给决测更可信和实用的依据。 IO的类型 平均响应时间直接关联到具体的IO类型： 读或写 单块或多块单块IO，指一次只读一个块。例如，当一个session等待一个单块IO时，典型的等待事件就是“db file sequential read”，表明正在等待需要的块。 多块读指的是一次读多个块，从2到128个Oracle块不等，依赖于块的大小与操作系统设置。通常一个多块请求容量上有1MB的限制。例如当一个session等待一次多块IO时，典型的等待事件就是“db file scattered read”，表明正在等待需要的块。 同步或异步同步(阻塞)操作等待硬件完成物理IO，完成后能得到通知，合理地管理操作的成功或失败(成功读的情况下可以接收结果)。当需要等待系统调用结果的时候，进程的执行是被堵塞的。 对于异步(非阻塞)操作，一旦IO请求传递到硬件，或放入操作系统的队列中(典型的情况是物理IO开始之前)，系统调用会立即返回。进程的执行不会被堵塞，因为它不需要等待系统调用的结果。它能继续执行，当IO操作有结果时再接收。 响应时间的阈值 一次典型的多块同步读64x 8k(总计512KB)的平均时间应该在未出现IO变慢的情况下大约是20毫秒左右。小请求应该更快(10-20毫秒)，大请求的消耗时间应该不多于25毫秒。异步操作应该至少和同步操作一样快，甚至还要更快。单块读至少应该和多块读一样快，甚至还要更快。“log file parallel write”，“control file write”和“direct path writes“等待时间应该不多于15毫秒。数据文件写的测量不像读那样简单。DBWR以批量的方式(&quot;db file parallel write&quot;)异步写入块，现在还没有写操作响应时间的标准。如果DBWR(多块或单块，带或不带IO salves)足够快速能够清理脏块，那么其他的等待事件和统计信息就会显露出来。作为规则，超过上述等待事件时间的等待事件都应该详细分析，当对比之前的时间消耗，有明显变化时更需要知晓。 注意：当系统低于这些最大阈值的时候，并不意味着没有其他的调优方法。 响应时间因系统而有所不同。例如，接下来的几项内容可以看做是正常平均值： 多块同步读时间是10毫秒。 单块同步读时间是5毫秒。 'log file parallel write'时间是3毫秒。 以上是基于多块IO比单块IO需要更多的IO子系统资源的前提。如果接受这些建议，redo日志最好放在最快的磁盘，并且没有其它并发活动的争用。 以下是各IO相关等待事件的响应时间阈值： Wait Event R/W Synchronous/ Asynchronous Singleblock/ Multiblock Elapsed Time control file parallel write Write Asynchronous Multi &lt; 15ms control file sequential read Read Synchronous Single &lt; 20 ms db file parallel read Read Asynchronous Multi &lt; 20 ms db file scattered read Read Synchronous Multi &lt; 20 ms db file sequential read Read Synchronous Single &lt; 20 ms direct path read Read Asynchronous Multi &lt; 20 ms direct path read temp Read Asynchronous Multi &lt; 20 ms direct path write Write Asynchronous Multi &lt; 15 ms direct path write temp Write Asynchronous Multi &lt; 15 ms log file parallel write Write Asynchronous Multi &lt; 15 ms 确定响应时间的途径 10046 trace file 当在10046 trace中使用level 8或12时，会包含相关的等待事件的信息，响应时间是ela字段，单位是微秒。 WAIT #5: nam='cell single block physical read' ela= 672 cellhash#=2520626383 diskhash#=1377492511 bytes=16384 obj#=63 tim=1280416903276618 &gt;&gt; 672 microseconds = 0.672 ms WAIT #5: nam='db file sequential read' ela= 1018 file#=2 block#=558091 blocks=1 obj#=0 tim=10191852599110 &gt;&gt; 1018 microseconds =&gt; 1.018 ms System State Dump 对于每个系统级的进程，等待信息包括在进程信息中。通常显示一个活动的waiting for，或者等待完成，进程正在CPU中执行waited for/last wait for。 其中waiting for表示进程处于等待状态。11g之前可以查看seconds since wait started字段，显示进程已经等待多久了。从11gR1开始，”total“字段显示等待的时间。 如果waiting for显示一个进程正在等待一个IO相关的操作，seconds since wait started&gt;0，表示可能IO丢失，session处于hang状态。(因为之前提到过平均可接受时间是20毫秒，任何IO等待时间超过1秒都需要关注)。 last wait for是与11g之前的版本相关的，表明进程不在等待(例如正在使用CPU)。等待时间记录到”wait_time“字段。(11g中wait_time被not in wait替代) last wait for 'db file sequential read' blocking sess=0x0 seq=100 wait_time=2264 seconds since wait started=0 file#=45, block#=17a57, blocks=1 &gt;&gt; 2264 microseconds =&gt; 2.264 ms waited for表示session不在等待。通常是11gR1以后的系统级trace中使用。total字段表示等待的总时间。 0: waited for 'db file sequential read' file#=9, block#=46526, blocks=1 wait_id=179 seq_num=180 snap_id=1 wait times: snap=0.007039 sec, exc=0.007039 sec, total=0.007039 sec wait times: max=infinite wait counts: calls=0 os=0 &gt;&gt; 0.007039 sec =&gt; 7.039 ms Statspack 与AWR reports 前台进程和后台进程的等待事件，平均响应时间通过Wait Avg(ms)反映(以毫秒计算的平均读)。 表空间IO 平均响应时间通过Av Rd (ms)反映(以毫秒计算的平均读)。 等待事件直方图可以提供组成这些平均值的写操作时间分布。他会展示出所有写操作都接近于平均值，还是会有若干波峰或波谷的情况。每列都表明每个bucket之间等待事件时间分布的百分比。例如，&lt;16ms的等待大于&lt;8ms。只要最大的百分比是从&lt;1ms到16ms的范围内，那么IO性能通常就可以接受。 4. 总结 本文的目标不是为了排查IO变慢的原因，而是为了如何寻找判定IO慢的证据。如果性能变差，那么IO慢可能成为性能问题的一个潜在原因，需要从数据库角度来分析如何采集支持的证据；如果潜在原因是由于操作系统级别的IO慢，那么负责IO子系统工程师需要参与进来诊断和修复这个问题。 ","link":"https://hkxtor.github.io/vQKm4MfiK/"}]}